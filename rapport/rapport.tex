\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{minted}

\begin{document}

%title
\begin{titlepage}
	\vspace{-20px}
	\begin{tabular}{l}
		\textsc{Blin} Sébastien\\
		\textsc{Drouin Viallard} Victor
	\end{tabular}
	\hfill \vspace{10px}\includegraphics[scale=0.1]{uqac}\\
	\vfill
	\begin{center}
		\Huge{Université du Quebec à Chicoutimi}\\
		\vspace{1cm}
		\LARGE{Maîtrise professionnelle}\\
		\large{Parcours Informatique}\\
		\vspace{0.5cm}\hrule\vspace{0.5cm}
		\LARGE{\textbf{TP 1 8INF846}}\\
		\Large{Réalisation d'un agent intelligent}
		\vspace{0.5cm}\hrule
		\vfill
		\vfill
	\end{center}
	\begin{flushleft}
		\Large{Sous l'encadrement de~:}\\
		\vspace{0.2cm}
		\large{\textsc{Bouchard} Kévin}
	\end{flushleft}
	\vfill
\end{titlepage}

\section{Introduction}
Le but de ce premier TP était de réaliser un agent intelligent. Cet agent prend la forme d'un aspirateur qui doit aspirer les pièces d'un manoir où de la poussière et des bijoux apparaissent de manière aléatoire. De plus, l'agent doit posséder un état mental de type BDI comme vu en cours. Dans ce rapport, nous verrons dans une première partie comment exécuter le programme, puis en second temps nous justifierons nos choix de modélisation.
\section{Exécution du programme}
\subsection{Dépendances du projet}
Ce devoir a été développé en \emph{C++14} et a été testé sur des environnements \emph{Linux} (mais fonctionne normalement sur Windows). Pour compiler le programme il vous faut~:
\begin{itemize}
    \item \emph{G++} (version 6 recommandée)
    \item La librairie \emph{SFML}
    \item \emph{Make}
\end{itemize}
\subsection{Compilation}
Pour compiler le programme, il suffit d'exécuter la commande \emph{make}. Un binaire nommé \emph{agentTP1} sera alors présent.
\subsection{Configuration et exécution}
Le fichier possède 2 fichiers de configuration. Le premier décrit la carte dans laquelle l'agent va évoluer. Par exemple, voici le fichier de configuration qui correspond à la carte du TP1 (d'autres cartes sont disponibles dans le répertoire)~:
\begin{minted}{python}
11100
11111
11100
\end{minted}

Le second fichier configure l'agent. On peut lui donner le nom de la stratégie souhaitée (pour le moment deux stratégies ont été implémentées, \emph{SuckWithLevel} et \emph{State}) ainsi que sa position de base dans la carte~:

\begin{minted}{python}
###############################################################################
#                              VACUUM PARAMETERS                              #
###############################################################################

# Strategy for the agent
# {SuckWithLevel}
# {State}
strategy=SuckWithLevel
base=0;0
# 0 = nothing, 1 = Choosed Action + Status, 2 = 1 + States process, 3 = 2 + internal map
log=4
speed=1
\end{minted}

Le paramètre log change le niveau d'affichage d'informations dans la console. Le paramètre speed modifie la vitesse d'éxécution, qui est aussi modifiable en cours d'éxécution avec les touches \emph{p} et \emph{m}.

Une fois ces deux fichiers créés, il suffit d'exécuter le programme~:
\begin{verbatim}
./agentTP1 map vacuum
\end{verbatim}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.4]{execution}
		\caption{Le programme en cours d'exécution}
		\label{fig:Execution}
	\end{center}
\end{figure}

\section{Structure du programme}
\subsection{Structure générale}
Comme demandé dans le sujet, notre programme s'exécute sur deux threads. Le premier thread réalise toute la gestion de la carte en créant de la poussière et des bijoux. Le second thread correspond à l'agent en observant l'environnement visible, puis donne le résultat à une stratégie qui en déduit la prochaine action et l'agent l'exécute.

Les fichiers intéressants pour la création et le lancement des threads sont \emph{main.cpp}, \emph{Loader.*} et \emph{Startable.*}.
\subsection{L'environnement}
La partie environnement est implémentée dans les fichier \emph{Map.*}. La carte est représentée par un vecteur de vecteur de Case (\emph{Case.h}) et est mise à jour dans la fonction \begin{minted}{cpp}
void Map::update(double delta);
\end{minted}
qui consiste en une boucle ou un évènement (soit un ajout de bijoux avec une probabilité de 0.2, soit un ajout de poussière avec une probabilité de 0.8) sur une case tirée au hasard. Deux évènements sont espacés d'un temps aléatoire (entre 10 et 1000 ms).
\subsection{L'aspirateur}
L'agent reprend le fonctionnement donné en pseudo-code dans le sujet du TP, c'est-à-dire~:
\begin{minted}{cpp}
While (amIAlive()){
    ObserveEnvironmentWithAllMySensors();
    UpdateMyState();
    ChooseAnAction();
    justDoIt();
}
\end{minted}
En effet, tout comme \emph{Map}, on peut trouver la fonction~:
\begin{minted}{cpp}
void Vacuum::update(double delta);
\end{minted}
qui observera l'environnement en mettant à jour une structure \emph{Sensors} si la batterie de l'agent n'est pas vide via la fonction~:
\begin{minted}{cpp}
Sensors Vacuum::observe();
\end{minted}
L'objet \emph{Sensors} est décrit dans \emph{Sensors.h} et contient le niveau de poussière et le nombre de bijoux de la pièce, ainsi que la possibilité de se déplacer dans les pièces voisines. Cette captation est passée en paramètres à la stratégie qui s'occupera de mettre à jour l'état interne et de donner la prochaine action à exécuter (donc \emph{UpdateMyState()} et \emph{ChooseAnAction()} qui seront expliquées plus bas). Enfin, l'agent exécute l'Action retournée par la stratégie via la fonction ~:
\begin{minted}{cpp}
void Vacuum::executeCurrentAction(double delta);
\end{minted}
\subsection{Les stratégies implémentées}

Deux stratégies ont été implémentées pour ce TP. Une première (\emph{SuckWithLevelStrategy}) tente d'évaluer la meilleure action possible et ne possède pas d'algorithme de déplacement (et donc ne marche pas sur une carte avec des obstacles, comme peut le montrer un test sur le fichier \emph{map2}). Une seconde (\emph{StateStrategy}) fonctionne à l'aide d'une machine à état et d'un algorithme pour planifier ses trajectoires (pour fonctionner sur les cartes avec obstacles).

\subsubsection{SuckWithLevelStrategy}

L'intelligence de l'agent est implémentée dans une stratégie. Nous avons modélisé un agent basé sur les buts avec un état mental de type \textsc{bdi} dans les fichiers \emph{SuckWithLevelStrategy.*}. Les croyances de l'agent sont représentées par une map interne qui donne ce que sais l'agent à un moment T. Cette carte est mise à jour avant chaque décision via la fonction~:
\begin{minted}{cpp}
void SuckWithLevelStrategy::updateInternalMap(const Sensors& sensors);
\end{minted}
et est représentée via un \emph{std::deque<std::deque<StrCase>>} (plus facile à transformer lors de la découverte de la carte par l'agent). Une \emph{StrCase} est une Case avec l'attribut~:
\begin{minted}{cpp}
std::time_t lastVisit;
\end{minted}

Le désir de l'agent est de rendre la carte la plus propre possible en évitant d'aspirer les bijoux (mais en les ramassant), tout en évitant de tomber en panne. Pour modéliser ces choix, une fonction s'occupe de donner un score à chaque action possible. Le but de l'agent est de maximiser son score. Cette prise de décision est modélisée dans la fonction~:
\begin{minted}{cpp}
Action SuckWithLevelStrategy::findNextAction(const Sensors& sensors);
\end{minted}
Ainsi, l'action retournée correspond à son intention. Et on peut voir 4 intentions différentes~:
\begin{itemize}
    \item Nettoyer la poussière de la case quand l'agent pense qu'il y a assez de poussière
    \item Ramasser les bijoux, dans ces 2 premiers cas le désir et l'intention sont identiques
    \item Se déplacer vers la case qu'on a visité il y a le plus longtemps (ou il y a des chances d'avoir assez de poussière maintenant)
    \item Se déplacer vers la case d'origine ou on peut se recharger. Dans ces deux derniers cas, I=la prochaine case visitée, D=Le chemin à parcourir pour se rendre à cette case. Bien sûr, sur la route, l'agent peut décider d'aspirer/ramasser les bijoux s'il le juge utile.
\end{itemize}

\subsubsection{StateStrategy}

Tout comme la précédente stratégie, \emph{StateStrategy} possède une fonction pour mettre à jour sa carte interne représentée par une \emph{std::deque<std::deque<StrCase>>}~:
\begin{minted}{cpp}
void updateInternalMap(const Sensors& sensors);
\end{minted}
La principale différence vient du choix d'action qui est donné par une machine à état (visible dans \emph{StateStrategyStates.*}). Tout comme la première stratégie on peut voir 4 intentions différentes. La seconde différence vient de la planification de la trajectoire. En effet, la première stratégie n'a pas d'algorithme de déplacement, mais se contente de se rapprocher de sa case objectif. Si un obstacle est présent, le robot risque de rester dans le même coin. Ici, la planification de la trajectoire est implémentée à l'aide d'un \emph{A*} dans la fonction~:
\begin{minted}{cpp}
	std::vector<ActionType> StateStrategy::pathTo(const Pos& target);
\end{minted}


\section{Conclusion}
Au final, ce projet nous a permis d'implémenter plusieurs fonctionnements décrits en cours. Tout d'abord le concept d'agent ou nous avons dû réfléchir à quel type d'agent notre aspirateur correspondait. Ensuite, le principe d'état mental BDI. Enfin, comment implémenter la prise de décision à l'aide d'un scoring des différentes actions.

\end{document}
