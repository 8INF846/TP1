\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{minted}

\begin{document}

%title
\begin{titlepage}
	\vspace{-20px}
	\begin{tabular}{l}
		\textsc{Blin} Sébastien\\
		\textsc{Drouin Viallard} Victor
	\end{tabular}
	\hfill \vspace{10px}\includegraphics[scale=0.1]{uqac}\\
	\vfill
	\begin{center}
		\Huge{Université du Quebec à Chicoutimi}\\
		\vspace{1cm}
		\LARGE{Maîtrise professionnelle}\\
		\large{Parcours Informatique}\\
		\vspace{0.5cm}\hrule\vspace{0.5cm}
		\LARGE{\textbf{TP 1 8INF846}}\\
		\Large{Réalisation d'un agent intelligent}
		\vspace{0.5cm}\hrule
		\vfill
		\vfill
	\end{center}
	\begin{flushleft}
		\Large{Sous l'encadrement de~:}\\
		\vspace{0.2cm}
		\large{\textsc{Bouchard} Kévin}
	\end{flushleft}
	\vfill
\end{titlepage}

\section{Introduction}
Le but de ce premier TP était de réaliser un agent intelligent. Cet agent prend
la forme d'un aspirateur qui doit aspirer les pièces d'un manoir où de la
poussière et des bijoux apparaissent de manière aléatoire. De plus, l'agent doit
posséder un état mental de type BDI comme vu en cours. Dans ce rapport, nous
verrons dans une première partie comment exécuter le programme, puis en second
temps nous justifierons nos choix de modélisation.
\section{Exécution du programme}
\subsection{Dépendances du projet}
Ce devoir a été développé en \emph{C++14} et a été testé sur des environnements
\emph{Linux} (mais fonctionne normalement sur Windows). Pour compiler le
programme il vous faut~:
\begin{itemize}
    \item \emph{G++} (version 6 recommandée)
    \item La librairie \emph{SFML}
    \item \emph{Make}
\end{itemize}
\subsection{Compilation}
Pour compiler le programme, il suffit d'exécuter la commande \emph{make}. Un
binaire nommé \emph{agentTP1} sera alors présent.
\subsection{Configuration et exécution}
Le dossier des sources possède 2 types de fichiers de configuration. Le premier
permet de décrire la carte dans laquelle l'agent va évoluer. Voici un exemple de
configuration correspondant à la carte du sujet du TP1~:
\begin{minted}{python}
11100
11111
11100
\end{minted}
Deux exemplaires de fichiers de configuration de carte sont présents et nommés
mph{map} et mph{map2}.

Le second type de fichier configure l'agent. On peut lui donner le nom de la
stratégie souhaitée (pour le moment deux stratégies ont été implémentées,
\emph{SuckWithLevel} et \emph{State}) ainsi que sa position initiale sur la
carte (cette position sera aussi la position de sa base de recharge)~:

\begin{minted}{python}
###############################################################################
#                              VACUUM PARAMETERS                              #
###############################################################################

# Strategy for the agent
# {SuckWithLevel}
# {State}
strategy=SuckWithLevel
base=0;0
# 0 = nothing, 1 = Choosed Action + Status, 2 = 1 + States process, 3 = 2 + internal map
log=4
speed=1
\end{minted}

Le paramètre log change le niveau d'affichage d'informations dans la console. Le
paramètre speed modifie la vitesse d'éxécution, qui est aussi modifiable en
cours d'éxécution avec les touches \emph{p} et \emph{m}. Deux exemples de tels
fichiers sont présents et nommés mph{vacuum} et mph{vacuum2}.


Une fois ces deux fichiers créés, il suffit d'exécuter le programme~:
\begin{verbatim}
./agentTP1 map vacuum
\end{verbatim}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.4]{execution}
		\caption{Le programme en cours d'exécution}
		\label{fig:Execution}
	\end{center}
\end{figure}

\section{Structure du programme}
\subsection{Structure générale}
Comme demandé dans le sujet, notre programme s'exécute sur deux threads
principaux. Le premier thread réalise toute la gestion de la carte en créant de
la poussière et des bijoux. Le second thread correspond à l'agent en observant
l'environnement visible, puis donne le résultat à une stratégie qui en déduit la
prochaine action et l'agent l'exécute. Un troisième thread est présent et gère
l'affichage des graphique des données.

Les fichiers intéressants pour la création et le lancement des threads sont
\emph{main.cpp}, \emph{Loader.*} et \emph{Startable.*}.
\subsection{L'environnement}
La partie environnement est implémentée dans les fichier \emph{Map.*}. La carte
est représentée par un vecteur de vecteur de cases (\emph{Case.h}) et est mise à
jour dans la fonction \begin{minted}{cpp}
  void Map::update(double delta);
\end{minted}
qui consiste en une boucle où un évènement à réalisé est choisi (soit un ajout
de bijoux avec une probabilité de 0.2, soit un ajout de poussière avec une
probabilité de 0.8) sur une case tirée au hasard. Deux évènements sont espacés
d'un temps aléatoire (entre 100 et 1000 millisecondes).
\subsection{L'aspirateur}
L'agent reprend le fonctionnement donné en pseudo-code dans le sujet du TP,
c'est-à-dire~:
\begin{minted}{cpp}
While (amIAlive()){
    ObserveEnvironmentWithAllMySensors();
    UpdateMyState();
    ChooseAnAction();
    justDoIt();
}
\end{minted}
Tout comme \emph{Map}, on peut trouver la fonction~:
\begin{minted}{cpp}
void Vacuum::update(double delta);
\end{minted}
puisque les deux classes héritent de \emph{Startable}. Cette fois la méthode
observera l'environnement en mettant à jour l'état observé via la méthode~:
\begin{minted}{cpp}
Sensors Vacuum::observe();
\end{minted}
L'objet \emph{Sensors} est décrit dans \emph{Sensors.h} et contient le niveau de
poussière et le nombre de bijoux de la case actuelle, ainsi que la possibilité
de se déplacer dans les cases voisines. Cette captation est passée en paramètres
à la stratégie pour l'empêcher de tricher en modifiant directement l'état de
l'environnement. Elle s'occupera de mettre à jour l'état interne et dedonner la
prochaine action à exécuter (donc \emph{UpdateMyState()} et
\emph{ChooseAnAction()} qui seront expliquées plus bas). Enfin, l'agent exécute
l'Action retournée par la stratégie via la fonction ~:
\begin{minted}{cpp}
void Vacuum::executeCurrentAction(double delta);
\end{minted}
\subsection{Les stratégies implémentées}

Deux stratégies ont été implémentées pour ce TP. Une première
(\emph{SuckWithLevelStrategy}) tente d'évaluer la meilleure action possible et
ne possède pas d'algorithme de déplacement (et donc ne fonctionne pas bien sur
une carte avec des obstacles complexes, comme peut le montrer un test sur le
fichier \emph{map2}). Une seconde (\emph{StateStrategy}) fonctionne à l'aide
d'une machine à état et d'un algorithme pour planifier ses trajectoires de
façon à fonctionner même sur les cartes avec obstacles complexes.

\subsubsection{SuckWithLevelStrategy}

L'intelligence de l'agent est implémentée dans une stratégie. Nous avons modélisé un agent basé sur les buts avec un état mental de type \textsc{bdi} dans les fichiers \emph{SuckWithLevelStrategy.*}. Les croyances de l'agent sont représentées par une map interne qui donne ce que sais l'agent à un moment T. Cette carte est mise à jour avant chaque décision via la fonction~:
\begin{minted}{cpp}
void SuckWithLevelStrategy::updateInternalMap(const Sensors& sensors);
\end{minted}
et est représentée via un \begin{verbatim}std::deque<std::deque<StrCase>>\end{verbatim} (plus facile à transformer lors de la découverte de la carte par l'agent). Une \emph{StrCase} est une Case avec l'attribut~:
\begin{minted}{cpp}
std::time_t lastVisit;
\end{minted}

Le désir de l'agent est de rendre la carte la plus propre possible en évitant d'aspirer les bijoux (mais en les ramassant), tout en évitant de tomber en panne. Pour modéliser ces choix, une fonction s'occupe de donner un score à chaque action possible. Le but de l'agent est de maximiser son score. Cette prise de décision est modélisée dans la fonction~:
\begin{minted}{cpp}
Action SuckWithLevelStrategy::findNextAction(const Sensors& sensors);
\end{minted}
Ainsi, l'action retournée correspond à son intention. Et on peut voir 4 intentions différentes~:
\begin{itemize}
    \item Nettoyer la poussière de la case quand l'agent pense qu'il y a assez de poussière
    \item Ramasser les bijoux, dans ces 2 premiers cas le désir et l'intention sont identiques
    \item Se déplacer vers la case qu'on a visité il y a le plus longtemps (ou il y a des chances d'avoir assez de poussière maintenant)
    \item Se déplacer vers la case d'origine ou on peut se recharger. Dans ces deux derniers cas, I=la prochaine case visitée, D=Le chemin à parcourir pour se rendre à cette case. Bien sûr, sur la route, l'agent peut décider d'aspirer/ramasser les bijoux s'il le juge utile.
\end{itemize}

\subsubsection{StateStrategy}

Tout comme la précédente stratégie, \emph{StateStrategy} possède une fonction pour mettre à jour sa carte interne représentée par une \emph{std::deque<std::deque<StrCase>>}~:
\begin{minted}{cpp}
void updateInternalMap(const Sensors& sensors);
\end{minted}
La principale différence vient du choix d'action qui est donné par une machine à
état (visible dans \emph{StateStrategyStates.*}). Tout comme la première
stratégie on peut voir 4 intentions différentes représentées par les 4 états
implémentés. Lorsque la stratégie doit calculer l'action à réaliser, elle
demande à son état courant. Si son état retourne une action valide (autre que
None) alors c'est l'action à effectuer. Sinon (l'action retournée est None)
alors c'est que la stratégie a changé d'état et il faut demander au nouvel état
l'action à réaliser.
La seconde différence vient de la planification de la trajectoire. En effet, la
première stratégie n'a pas d'algorithme de déplacement, mais se contente de se
rapprocher de sa case objectif. Si un obstacle est présent, le robot risque de
rester dans le même coin. Ici, la planification de la trajectoire est
implémentée à l'aide d'un algorithme \emph{A*} dans les méthodess~:
\begin{minted}{cpp}
  std::vector<ActionType> StateStrategy::pathTo(const Pos& target);
  std::vector<ActionType> StateStrategy::constructpath(...);
\end{minted}

En outre, cette stratégie aspire les cases pas à pas (restant au maximum 0.2s par
aspiration) pour vérifier de manière régulière si un bijou ne serait pas tombé
sur la case actuelle depuis la dernière observation de l'environnement.


\section{Conclusion}
Au final, ce projet nous a permis d'implémenter plusieurs fonctionnements
décrits en cours. Tout d'abord le concept d'agent ou nous avons dû réfléchir à
quel type d'agent notre aspirateur correspondait. Ensuite, le principe d'état
mental BDI. Enfin, comment implémenter la prise de décision à l'aide d'un
scoring des différentes actions, ou d'une machine à états finie.

\end{document}
